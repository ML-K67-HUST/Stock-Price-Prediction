### K-Nearest-Neighbors Algorithm

The K-Nearest Neighbors algorithm, commonly known as KNN, is a non-parametric approach where the response of a data point is determined by the nature of it
k neighbors from the training set. It can be used in both classification and regression settings. KNN uses the entire training set, no training is require.
For the regression the output can be the mean, while for the classification the output can be the most common class.

![image](https://github.com/ktuanPT373/STOCK-PRICE-PREDICTION/blob/main/Script%20for%20making%20slides/Basic_KNN1.png)

![image](https://github.com/ktuanPT373/STOCK-PRICE-PREDICTION/blob/main/Script%20for%20making%20slides/Basic_KNN2.png)

### Calculate the distance between points

Several method can be used to calcukate the distance between points in the dataset, for example:

![image](https://github.com/ktuanPT373/STOCK-PRICE-PREDICTION/blob/main/Script%20for%20making%20slides/Euclid%20Distance.png)

![image](https://github.com/ktuanPT373/STOCK-PRICE-PREDICTION/blob/main/Script%20for%20making%20slides/Manhattan%20Distance.png)

### How does the KNN for Regression work?
1. First, the distance between the new point and each training point is calculated.

![image](https://github.com/ktuanPT373/STOCK-PRICE-PREDICTION/blob/main/Script%20for%20making%20slides/Work_1.png)

2.The closest k data points are selected (based on the distance). In this example, points 1, 5, and 6 will be selected if the value of k is 3. We will further explore the method to select the right value of k later in this article.

![image](https://github.com/ktuanPT373/STOCK-PRICE-PREDICTION/blob/main/Script%20for%20making%20slides/Work_2.png)

3.The average of these data points is the final prediction for the new point.

*NOTE: The best k value must be found by testing. For a very low value of k (suppose k=1), the model is overfitting the training data, which leads to a high error rate on the validation set. On the other hand, for a high value of k, the model performs poorly on both the train and validation sets.

### Implementation of KNN

1. Read the file:

![image](https://github.com/ktuanPT373/STOCK-PRICE-PREDICTION/blob/main/Script%20for%20making%20slides/Read%20file.png)

Here we use Microsoft dataset.

2. Split the dataset into train set and test set:

![image](https://github.com/ktuanPT373/STOCK-PRICE-PREDICTION/blob/main/Script%20for%20making%20slides/Split%20dataset.png)

3. Use model to predict the stock price:

![image](https://github.com/ktuanPT373/STOCK-PRICE-PREDICTION/blob/main/Script%20for%20making%20slides/Use%20model%20to%20predict%20the%20stock%20price.png)
![image](https://github.com/ktuanPT373/STOCK-PRICE-PREDICTION/blob/main/Script%20for%20making%20slides/Use%20model%20to%20predict%20the%20stock%20price1.png)

4.Evaluation:

![image](https://github.com/ktuanPT373/STOCK-PRICE-PREDICTION/blob/main/Script%20for%20making%20slides/Plot.png)

![image](https://github.com/ktuanPT373/STOCK-PRICE-PREDICTION/blob/main/Script%20for%20making%20slides/Evaluation.png)

=> This is the best result among the 4 models we trained.

### Pro and Con

1.Advantages:

-Effective if data is large

-No learning phase

-Robust to noisy data, no need to filter outliers

2.Disadvantage:

-Due to KNN is a non-parameter model, it must be trained with the entire data, so the implementation takes a long time.

-KNN is sensible to the curse of dimensionality
